#### BIO

BIO全称是Blocking IO，是JDK1.4之前的传统IO模型，本身是同步阻塞模式。 线程发起IO请求后，一直阻塞IO，直到缓冲区数据就绪后，再进入下一步操作。针对网络通信都是一请求一应答的方式，虽然简化了上层的应用开发，但在性能和可靠性方面存在着巨大瓶颈，试想一下如果每个请求都需要新建一个线程来专门处理，那么在高并发的场景下，机器资源很快就会被耗尽。

#### NIO

NIO也叫Non-Blocking IO 是同步非阻塞的IO模型。线程发起io请求后，立即返回（非阻塞io）。同步指的是必须等待IO缓冲区内的数据就绪，而非阻塞指的是，用户线程不原地等待IO缓冲区，可以先做一些其他操作，但是要定时轮询检查IO缓冲区数据是否就绪。Java中的NIO 是new IO的意思。其实是NIO加上IO多路复用技术。普通的NIO是线程轮询查看一个IO缓冲区是否就绪，而Java中的new IO指的是线程轮询地去查看一堆IO缓冲区中哪些就绪，这是一种IO多路复用的思想。IO多路复用模型中，将检查IO数据是否就绪的任务，交给系统级别的select或epoll模型，由系统进行监控，减轻用户线程负担。

NIO主要有buffer、channel、selector三种技术的整合，通过零拷贝的buffer取得数据，每一个客户端通过channel在selector（多路复用器）上进行注册。服务端不断轮询channel来获取客户端的信息。channel上有connect,accept（阻塞）、read（可读）、write(可写)四种状态标识。根据标识来进行后续操作。所以一个服务端可接收无限多的channel。不需要新开一个线程。大大提升了性能。

#### AIO

AIO是真正意义上的异步非阻塞IO模型。 上述NIO实现中，需要用户线程定时轮询，去检查IO缓冲区数据是否就绪，占用应用程序线程资源，其实轮询相当于还是阻塞的，并非真正解放当前线程，因为它还是需要去查询哪些IO就绪。而真正的理想的异步非阻塞IO应该让内核系统完成，用户线程只需要告诉内核，当缓冲区就绪后，通知我或者执行我交给你的回调函数。

AIO可以做到真正的异步的操作，但实现起来比较复杂，支持纯异步IO的操作系统非常少，目前也就windows是IOCP技术实现了，而在Linux上，底层还是是使用的epoll实现的。



### select、poll、epoll 

##### https://zhuanlan.zhihu.com/p/55269256

select：主要就是起一个监听的作用。也就是监听多个文件描述符。

在多线程多进程情况时，我们是这样处理连接进来的socket的：

![img](https://pic3.zhimg.com/80/v2-49e0d29ba45efa4d359681c647063e5a_1440w.jpg)

中间的数组是文件描述符集合，里面都是socket的文件描述符。然后一个线程或进程调用accept进行阻塞监听，然后来了一个连接后，就创建新的进程或者线程去对应该连接，然后就有了新的文件描述符。新的文件描述符就是负责和连接通信的，比如读写什么的，就不是accept新的进程线程了。这种情况下，当客户端没有发数据来服务端，那么对应的线程和进程会一直处于阻塞状态，它要读取该文件描述符，可是却数据，所以一直不返回。而很多情况下，客户端和服务端的数据量传输都是比较少的，所以就有大量的进程或线程处于阻塞状态，这样就会非常耗费系统的资源。

上面就是没有select，poll，epoll的时候的处理方式。

而select是这样的，如下图：

![img](https://pic4.zhimg.com/80/v2-4f5057199f22b90350bd748e89453b1b_1440w.jpg)

select只在一个线程内运行，但它可以监视多个文件描述符，这样就不需要另外的进程和线程资源了。然后我们来叙述一下select的一个比较完整的运行流程：如下图：

![img](https://pic3.zhimg.com/80/v2-0b8e3969800ba34c819104ea153543e6_1440w.jpg)

我来慢慢解释：socket文件描述符在这里分两种，一种是用于接收外面的socket的，另一种是和外面的socket进行通信的。比如上图中，文件描述符7就是用于接收请求连接的客户端，4，5，6，8都是直接和外面的socket通信的。我们先把4，5，6，7都挂载到select上，也就是说当文件描述符4，5，6，7任一个处于就绪状态的时候，就会反应到select上，什么时候文件描述符处于就绪状态呢？就是当外面发送数据进来，然后文件描述符这里有数据要读，或者文件描述符这边有请求连接的链接，那么select就会去遍历自己对应的文件描述符集合，看看到底是哪个文件描述符有反应了，**select返回的是就绪的文件描述符的个数**，如果是7有反应了，那么就调用accept函数去接收新的连接，然后分配新的文件描述符8给对应的连接，然后再把文件描述符8挂载到这个select上。如果是4，5，6上有反应了，那么就调用read函数，去读取文件描述符上的数据。当select返回后一般还会继续运行select，也就是说放在死循环里，然后继续处于阻塞状态，等待响应。 这里有个瓶颈：就是比如select在处理文件描述符4的时候，文件描述符5又来了数据，那么select就不能处理。这个看视频没有注意到答案？？？

这就是**多路IO转接**了，相信大家应该有理解了吧？如果大家对java的nio框架netty有所熟悉的话，应该就很容易理解了。虽然netty在linux上底层使用的是epoll，但是模型是一样的，有兴趣的可以了解一下：[netty学习](https://zhuanlan.zhihu.com/c_1040914558763941888) 。

这里要注意select的一个缺陷：

1.select能监听的文件描述符个数受限于FD_ SETSIZE,一 般为1024，单纯改变进程打开的文件描述符个数并不能改变select监听文件个数。（一个进程能打开的文件描述符的个数就是1024）

2.解决1024以下客户端时使用select是很合适的，但如果链接客户端过多，select采用的是轮询模型，会大大降低服务器响应效率，不应在select上投入更多精力



然后我们再来看看select函数的声明：

int select(int nfds, fd_ set *readfds, fd_ set *wr itefds,fd_ set *exceptfds, struct timeval *t imeout);

nfds:监控的文件描述符集里最大文件描述符加1,因为此参数会告诉内核检测前多少个文件描述符的状态

（比如刚才那副图中，文件描述符最大是8，所以这个值就是9）

readfds :监控有读数据到达文件描述符集合，传入传出参数

writefds :监控写数据到达文件描述符集合，传入传出参数

exceptfds :监控异常发生到达文件描述符集合,如带外数据到达异常，传入传出参数

（上面三个参数的意思就是：文件描述符有三种方式可以触发select的响应，需要读数据的时候，需要写数据的时候，发送异常的时候，比如说文件描述符4只存在于读数据的集合中，那么select只在4需要读数据的就绪情况下产生反应，其它情况都不产生反应，这样就显得更加灵活）

timeout :定时阻塞监控时间，3种情况

1.NULL，永远等下去

2.设置timeval,等待固定时间

3.设置timeval里时间均为0，检查描述字后立即返回，轮询

（跟java中的wait一个机制，语义理解就好）



所以，这个select就讲到这里了，学习这个能看懂别人的代码就好，或者用于局域网的几台机器即可，不需要要求太高。

（大家可以去看视频看看代码的演示效果，也就是使用select之后就可以一个进程线程对应多个进程线程的连接了）





poll:

就我的大致了解看来，和select没什么大的区别，就是这里没发现说没有最大连接数的限制了。

如下图：

![img](https://pic4.zhimg.com/80/v2-e3e36c9b14933b7b6825afc260170bb3_1440w.jpg)

第一感觉就是构造形式不同，它传入的是pollfd的结构体数组，一个结构体里，fd是文件描述符号，events是此文件描述符监控的事件，比如前面提到的读数据，写数据，异常。revents是返回的事件，比如此文件符监控读数据和写数据，那么读数据发生时，就会从这里返回。

感觉可能是自己现在的基础比较少，后面有了足够的基础后再来好好看看。



epoll:

epoll是Linux下多路复用IO接口se1ect/poll的增强版本，它能**显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率**，因为它会**复用文件描述符集合来传递结果而不用迫使开发者每次等待事件之前都必须重新准备要被侦听的文件描述符集合**，另一点原因就是获取事件的时候，它**无须遍历整个被侦听的描述符集，只要遍历那些被内核I0事件异步唤醒而加入Ready队列的描述符集合就行**了。

解释一下上面这段话：”复用文件描述符集合来传递结果而不用迫使开发者每次等待事件之前都必须重新准备要被侦听的文件描述符集合“ 也就是说，比如select函数响应返回之后，传入的参数，即监听的文件描述符集合需要重新准备，然后再传入。而epoll就不需要再重新准备传入的参数。

“无须遍历整个被侦听的描述符集，只要遍历那些被内核I0事件异步唤醒而加入Ready队列的描述符集合就行” 这句话的意思是：

如下图：

![img](https://pic1.zhimg.com/80/v2-2cd162d5f176bb06233ec3159aa0f448_1440w.jpg)

比如左边的就是传入的要监控的文件描述符集合。select和epoll返回了就绪的文件描述符的个数，但是不知道是哪个文件描述符，所以它需要遍历一遍去一一确认。而epoll就是用一个队列存储了这些就绪的文件描述符，直接遍历这个队列即可。这表示了什么？比如说有10万个要监控的文件描述符集合，然后只有三个文件描述符就绪了（或者说三个文件描述符是活跃的），如果是select或epoll,那么他们就需要遍历这10万个文件描述符（应该是满3个了就结束了吧，但是哪个就绪也不确定，如果是最后的那不就凉凉），这就消耗了很大的系统资源。而epoll只需要遍历这个只存储了三个元素的队列即可，这就跟开了挂一样。所以，这就显示出了它能**显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。**如果是10万的文件描述符，有8万的活跃文件描述符，那么epoll就显示不出比较大的优越性了。所以，并没有绝对的要使用哪个函数，取决于并发量和数据量。



目前epell是1inux大规模并发网络程序中的热门首选模型。

epoll除了提供select/ poll那种IO事件的电平触发(Leve1 Triggered) 外，还提供了边沿触发(Edge Triggered) ，这就使得用户空间程序有可能缓存IO状态，减少epoll_ wait/epoll_ pwait的调用，提高应用程序效率。（这个没怎么看得懂。。）



linux中查看可创建的最大的文件描述符个数的命令是：

cat /proc/sys/fs/file-max

我在阿里云看的是：

![img](https://pic1.zhimg.com/80/v2-c66c8275e5d8feef9dd2855d0072a80c_1440w.png)

不过是配置比较低的。

然后再来看看epoll的API：

1.创建一个epoll句柄，参数size用来告诉内核监听的文件描述符个数，跟内存大小有关

int epoll_ create(int size)

size :告诉内核监听的数目



其实就是创建一颗红黑树，方便文件描述符的插入，删除，查找。先创建一个节点，然后后面再慢慢插入，返回的是一个文件描述符，通过这个文件描述符就可以找到这颗红黑树，也就是这颗红黑树的头节点。树具有数组的快速查找特征，有具有链表的快速插入和删除的特征，至于为什么要选红黑树，可以参考：[数据结构和算法](https://zhuanlan.zhihu.com/c_1009736142882787328) 这个专栏里面的红黑树系列教程。

![img](https://pic4.zhimg.com/80/v2-11dad482f469eaca277a90643b7202af_1440w.jpg)





然后来看第二个api:

2.控制某个epoll监控的文件描述符上的事件:注册、修改、删除。

\#include <sys/epoll.h>

int epoll_ ctl(int epfd, int op, int fd, struct epoll event *event)

epfd:为epoll .creat的句柄**（也就是红黑树的头节点，说明这个文件描述符要插入哪颗红黑树上）**

op :表示动作，用3个宏来表示:

EPoLL CTL ADD(注册新的fd到epfd)，

EPOLL CTL MOD(修改已经注册的fd的监听事件)，

EPoLL CTL DEL(从epfd删除一一个fd) ;

（**也就对应了红黑树上的添加，修改，删除**）

fd:就是你要插入到这颗红黑树上的文件描述符

event : 告诉内核需要监听的事件，也就是前面说的写数据，读数据啥的

struct epoll _event {

__uint32_ t events; /* Epoll events

epoll data_ t data; /* User data variable

};

感觉有了前面的基础后，大家应该就很容易理解这个api了。第一个参数表明要插入到哪颗红黑树上，后面的参数都表示要插入的这颗节点的必备属性。大家理解起来应该没问题吧？我是基本上一遍就看懂的。



3.等待所监控文件描述符上有事件的产生，类似于select()调用。（前面两步就是准备好数据，这一步就是开启）

\#include <sys/epoll.h>

int epoll wait(int epfd, struct epoll event *events, int maxevents, int timeout )

epfd就是要开启的红黑树的头节点的文件描述符

events :用来从内核得到事件的集合，（也就是用一个指针接收从内核返回的存储就绪的队列）

maxevents : 告之内核这个events有多大，这个maxevents的值不能大于创建epoll create( )时的size,（肯定不会大于红黑树的数据量，不能太小，不然接收不完返回来的就绪队列数据。）

timeout :是超时时间

-1 :阻塞 0:立即返回，非阻塞>0 :指定微秒

返回值:成功返回有多少文件描述符就绪，时间到时返回0，出错返回-1



嗯，视频的内容就讲到这里了，有兴趣的可以去尝试运行一下那代码，我感觉我已经获得了需要的东西了。



有了前面这些基础后，我再去看看网上的其它链接的资料，得到了一些不同方面的认识。

如第二链接中所说：[select、poll、epoll之间的区别总结[整理\] + 知乎大神解答](https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq546770908/article/details/53082870)

**select的几大缺点：**

**（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大（因为在内核才能监听那些数据，也就是操作文件描述符的读写）**

**（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大**

**（3）select支持的文件描述符数量太小了，默认是1024**



poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多。

感觉复制不太好，还是截图吧，就当作笔记了。

![img](https://pic1.zhimg.com/80/v2-4a0e2948e47c619bc10c4ceb5119db88_1440w.jpg)

![img](https://pic3.zhimg.com/80/v2-c804d66ee794fdd113a7cdd003db6f8a_1440w.jpg)